---
hide:
  - navigation
  - toc
---

# Large Language Models Bootcamp

<div class="flex flex-col items-center justify-center dark-bg" markdown>

<div class="subtitle">ðŸš€ Announcing FSDL Large Language Models Bootcamp  ðŸš€</div>

<div style="align:center; text-align:center; width:50%" markdown>
On **April 21-22** join **250 other builders** and learn **best practices** and tools for **creating products with LLMs** from **expert educators** and **industry leaders** at an **in-person bootcamp** in **San Francisco**.
</div>

[Register now](#register){ .md-button .md-button--primary }

</div>

## Why

We are at the cusp of a technology unlock of a magnitude not seen since the early days of the Internet.

Large language models (LLMs) understand text both semantically and syntactically,
fulfilling the promise of an old idea in artificial intelligence and human-computer interaction:
_Natural Language User Interfaces_, or LUIs.

As with GUIs, LUIs promise to revolutionize how we use and interface with computing systems and become the specialty of a new class of developer.

At the same time that LLMs have made LUIs possible,
the process for developing an application around machine learning models has become easier.

Where once an idea for a useful, delightful, and intelligent technology
would bottleneck on training models from scratch
and then on deployment,
now an MVP based on pretrained models and APIs can be configured and serving users in an hour.

With a gold rush come shovels and shovel-sellers.
An entirely new ecosystem of tools and tool vendors has formed around LLMs and LUIs,
and even ML veterans are scrambling to orient themselves to the new possible
and figure out the most productive new sets of techniques and tools.

They are asking themselves:

- "Should I be able to code a Transformer from scratch?"
- "What is 'Prompt Engineering'? How legit is it?"
- "What's the deal with linear-time attention mechanisms?"
- "What will my moat be if Iâ€™m relying on OpenAI's model APIs?"
- "Are there open-source alternatives to LLM vendors?"
- "How do software engineering workflows like CI/CD and test-driven design transfer?"
- "How do computer science principles like compositionality, data structures, and recursion transfer?"
- "Jesus H. Christ, how am I supposed to test these things?"
- "Do I need to buy a cluster of server-grade GPUs like NVIDIA A100s?"
- "How can I ensure a seamless UX while still gathering user feedback to support a data flywheel?"

## What

We have put together a state-of-the-art program
based on emerging conventions in the LLM community and the latest research results
to help you answer these questions and make the transition.

Here are a few of the benefits of attendance:

- **Cloud Credits**
    - We're partnering with compute vendors to offer free cloud credits to all attendees, including credits for [Modal](https://modal.com), [banana.dev](https://banana.dev), and [LambdaLabs GPU Cloud](lambdalabs.com/cloud).

- **The askFSDL Sample Project**
    - Walk through a well-documented sample project using LLM APIs and frameworks, traditional and vector databases, and user feedback ingestion
    - Use as starter code to save hours of boilerplate effort on your own project

- **Demo Garden**
    - Bring a demo of an LLM-powered product and get valuable feedback from fellow learners and builders
    - Tour the demos to get inspiration for what's possible or see the next big project while it's still just a prototype

- **Foundations of Foundation Models**
    - Learn the core concepts behind transformer architectures, self-supervised learning, and text generation
    - Develop clear intuitions for model internals based on the latest work in "reverse engineering" LLMs

- **Learn to Spell: Effectively Using LLMs**
    - Learn how vendors, like OpenAI, Cohere, and AI21, compare with each other and with open source options like FLAN-T5 and GLM.
    - Prompt engineering tips and tricks: few-shot examples, chain-of-thought, formatting
    - Context engineering concepts: incorporating local information into LLM context, the wishlist-fulfillment architecture, long-term memory
    - Software tools: LangChain, GPTIndex, Everyprompt, dust.tt

- **Search 2.0**
    - How text embeddings enable semantic search everywhere
    - Choosing between vector stores, like FAISS, Milvus, Pinecone, Weaviate, and Vespa
    - Jointly embedding multiple types of data for multi-modal semantic search

- **Learning in Production**
    - Why "learn in prod" is the new "test in prod"
    - How to monitor models, trace chains, and record feedback
    - Methods for learning from user data, like reinforcement learning from human feedback (RLHF), and from chains of LLMs, like Constitutional AI

## Who

We are Full Stack Deep Learning.
We're a team of UC Berkeley PhD alumni
with years of industry experience
who are passionate about teaching people how to make deep neural networks work in the real world.

Since 2018, we have taught in-person bootcamps, online multi-week cohorts, and official semester-long courses at top universities.

<img src="/images/august2018.jpg" width="480px">

### Instructor Team

<div style="display:grid;grid-template-columns:47% 47%; row-gap: 2rem; column-gap:5%;">
  <div class="person" markdown>
    <img src="/images/charles.png" class="person--image" height="160px" width="160px" loading="lazy" alt="Photo of Charles Frye">
    <div><strong>Charles Frye</strong> educates people in AI. He has worked in the ML tooling space, including with companies such as Weights & Biases and Gantry, since getting a PhD in Theoretical Neuroscience at UC Berkeley.</div>
  </div>
  <div class="person" markdown>
    <img src="/images/sergey.png" class="person--image" height="160px" width="160px" loading="lazy" alt="Photo of Sergey Karayev">
    <div markdown>
    <strong>Sergey Karayev</strong> builds AI-powered products as Co-founder of Volition. He co-founded Gradescope after getting a PhD in AI at UC Berkeley.
    </div>
  </div>
  <div class="person" markdown>
    <img src="/images/josh.png" class="person--image" height="160px" width="160px" loading="lazy" alt="Photo of Josh Tobin">
    <div markdown>
    <strong>Josh Tobin</strong> builds tooling for AI products as Co-founder and CEO of Gantry. He worked as a Research Scientist at OpenAI and received a PhD in AI at UC Berkeley.
    </div>
  </div>
</div>

### Guest Talks

<div style="display:grid;grid-template-columns:47% 47%; row-gap: 2rem; column-gap:5%;">
  <div class="person" markdown>
    <img src="/images/harrison.jpg" class="person--image" height="160px" width="160px" loading="lazy" alt="Photo of Harrison Chase">
    <div><strong>Harrison Chase</strong> is the creator of LangChain</div>
  </div>
</div>

## When and Where

The event will be **in-person** and run **all day** on **Friday, April 21, 2023** and **Saturday, April 22, 2023** at the [South San Francisco Conference Center](https://ssfconf.com/).

<iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d74597.27001002253!2d-122.39849142057383!3d37.69637486488224!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x808f79b47d5d81ad%3A0xfd9c1f0af6155a3d!2sSouth%20San%20Francisco%20Conference%20Center!5e0!3m2!1sen!2sus!4v1674094089226!5m2!1sen!2sus" width="600" height="450" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade"></iframe>

## Sponsors

### LOGOS OF CONFIRMED SPONSORS

Interested in sponsoring?
You can read more about tiers and benefits [here](./sponsors).
Contact [`sponsorships@fullstackdeeplearning.com`](mailto:sponsorships@fullstackdeeplearning.com) with inquiries.

## Register

<div class="flex justify-center">
  <div class="tier">
    <div class="tier--header">Academics</div>
    <div class="tier--price">
      <span>$450</span>
    </div>
    <div style="margin-top: auto">
      <a href="https://fsdl.me/2023-conf-academic" style="width: 100%" class="md-button md-button--primary"> Apply for discount </a>
    </div>
  </div>
  <div class="tier">
    <div class="tier--header">Professionals</div>
    <div class="tier--price">
      <span>$950</span>
    </div>
    <div style="margin-top: auto">
      <a href="https://fsdl.me/2023-conf-irl-reg" style="width: 100%" class="md-button md-button--primary">Register</a>
    </div>
  </div>
  <div class="tier">
    <div class="tier--header">Investors</div>
    <div class="tier--price">
      <span>$2450</span>
    </div>
    <div style="margin-top: auto">
      <a href="https://fsdl.me/2023-conf-irl-reg" style="width: 100%" class="md-button md-button--primary">Register</a>
    </div>
  </div>

</div>

## Tentative Schedule

|       | Friday (April 21)                                             | Saturday (April 22)                      |
|-------|---------------------------------------------------------------|------------------------------------------|
| 9 am  | Registration & Breakfast                                      | Breakfast                                |
| 10 am | **Launch an LLM App in 1 Hour** (Charles)                     | **UX for LLMs** (Charles)                |
| 11 am | **Foundations of Foundation Models** (Charles)                | **Deploying LLM Apps** (Charles + Josh)  |
| 12 pm | Lunch and networking                                          | Lunch and networking                     |
| 1 pm  | **Learn to Spell: Effectively Using LLMs** (Charles + Sergey) | **Learning in Production** (Josh)        |
| 2 pm  | **Search 2.0** (Josh + Sergey)                                | **Future Directions** (Sergey)           |
| 3 pm  | Coffee and networking                                         | Coffee and demo garden                   |
| 4 pm  | Hands-On Workshop: askFSDL Project                            | **Harrison Chase**: Creator of LangChain |
| 5 pm  | Hands-On Workshop: askFSDL Project                            | Keynote                                   |
